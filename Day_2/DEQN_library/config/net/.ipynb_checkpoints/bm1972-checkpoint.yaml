layers:
  - hidden:
      units: 50
      type: dense
      activation: relu
      init_scale: 0.1
  - hidden:
      units: 50
      type: dense
      activation: relu
      init_scale: 0.1
  - output:
      type: dense
      activation: sigmoid
      init_scale: 0.1
