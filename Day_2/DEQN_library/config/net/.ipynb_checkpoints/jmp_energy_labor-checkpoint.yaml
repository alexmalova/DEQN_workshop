layers:
  - hidden:
     units: 512
     type: dense
     activation: relu
     init_scale: 1e-1 #it's converging faster with 1e-1 for every layer
  - hidden:
     units: 512
     type: dense
     activation: relu
     init_scale: 1e-1
  - output:
     type: dense
     activation: linear
     init_scale: 0.01

# --------------------------------------------------------------------------- #
# Mimic the Glorot uniform initializer using the VarianceScaling initializer
# --------------------------------------------------------------------------- #
# The Glorot normal initializer draws samples from a uniform distribution
# within [-limit, limit] where limit is sqrt(6 / (fan_in + fan_out)).
# The VarianceScaling method draws samples from a uniform distiruvtion within
# [-limit, limit], with limit = sqrt(3 * scale / n).
# When we set n as 'fan_avg' (fan_avg = (fan_in + fan_out) / 2) and scale=1,
# then the VarianceScaling method replicates the Glorot uniform initializer.

# "fan_in", "fan_out" or "fan_avg".
net_initializer_mode: fan_avg
# truncated_normal", "untruncated_normal" or "uniform".
net_initializer_distribution: truncated_normal
